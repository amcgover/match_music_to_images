{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Copyright 2021 Aonghus McGovern\n",
    "\n",
    "This file is part of Music_Image_Matcher_Notebooks_And_Writeup.\n",
    "\n",
    "Music_Image_Matcher_Notebooks_And_Writeup is free software: you can redistribute it and/or modify it under the terms of the \n",
    "GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) \n",
    "any later version.\n",
    "\n",
    "Music_Image_Matcher_Notebooks_And_Writeup is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; \n",
    "without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License \n",
    "for more details.\n",
    "\n",
    "You should have received a copy of the GNU General Public License along with Music_Image_Matcher_Notebooks_And_Writeup.  \n",
    "If not, see <https://www.gnu.org/licenses/>.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fileinput\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import operator\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from PIL import Image\n",
    "from scipy import stats\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "import time\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from Oramas et al.\n",
    "oramas_data = pd.read_csv('oramas_data_with_names.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two lists to hold the ratio of explained variance that PCA can explain for the content and style matrices\n",
    "# Once we have performed PCA on the content and style matrices for all of the images we will analyse these lists\n",
    "# to ensure that each image has at least 95% of variance explained\n",
    "content_ratios = list()\n",
    "style_ratios = list()\n",
    "\n",
    "# The below code is reworked from the \n",
    "# style transfer tutorial available here: https://pytorch.org/tutorials/advanced/neural_style_tutorial.html\n",
    "class Normalization(nn.Module):\n",
    "    def __init__(self, mean, std):\n",
    "        super(Normalization, self).__init__()\n",
    "        # .view the mean and std to make them [C x 1 x 1] so that they can\n",
    "        # directly work with image Tensor of shape [B x C x H x W].\n",
    "        # B is batch size. C is number of channels. H is height and W is width.\n",
    "        self.mean = torch.tensor(mean).view(-1, 1, 1)\n",
    "        self.std = torch.tensor(std).view(-1, 1, 1)\n",
    "\n",
    "    def forward(self, img):\n",
    "        # normalize img\n",
    "        return (img - self.mean) / self.std\n",
    "\n",
    "def get_content_matrix(target, name): \n",
    "    # This function gets the content matrix, resizes it using PCA and saves it\n",
    "    target = target.detach()\n",
    "    \"\"\"\n",
    "    The detach returns a 4-d matrix:\n",
    "    1 x 128 x 112 x 112.\n",
    "    PCA expects a 2-d array so we need to reshape it to\n",
    "    128 x 12544\n",
    "    12544 is 112*112\n",
    "    \"\"\"\n",
    "    reshaped = torch.reshape(target, (128, 12544))\n",
    "    # Create a PCA object and fit it to the reshaped array           \n",
    "    pca = PCA(n_components=50)\n",
    "    pca.fit(reshaped)\n",
    "    # Append the explained variance to the content_ratios list\n",
    "    content_ratios.append(sum(pca.explained_variance_ratio_))\n",
    "    # Create the reduced form of the reshaped matrix and save it\n",
    "    reduced = pca.fit_transform(reshaped)\n",
    "    pickle.dump(reduced, open('./content_matrices_50_pca/%s' % name, 'wb'))\n",
    "    return reduced\n",
    "    \n",
    "def get_style_matrix(target_feature, name):\n",
    "    # This function gets the style matrix, resizes it using PCA and saves it\n",
    "    target = gram_matrix(target_feature).detach()\n",
    "    # Create a PCA object and fit it to the style array\n",
    "    pca = PCA(n_components=15)\n",
    "    pca.fit(target)\n",
    "    # Append the explained variance to the style_ratios list\n",
    "    style_ratios.append(sum(pca.explained_variance_ratio_))\n",
    "    # Create the reduced form of the style matrix and save it            \n",
    "    reduced = pca.fit_transform(target)\n",
    "    pickle.dump(reduced, open('./style_matrices_15_pca/%s' % name, 'wb'))\n",
    "    return reduced\n",
    "\n",
    "def get_image(image_path, reshape_size = (224,224)):\n",
    "    # This function loads an image so that it can be converted to content and style matrices\n",
    "    transform = transforms.Compose([transforms.Resize(reshape_size),\n",
    "    transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "    \n",
    "    image = Image.open(open(image_path, 'rb'))\n",
    "    # Most of the images open in RGB mode. If an image doesn't open in RGB mode applying the functions to get \n",
    "    # content and style matrices give errors so we return None for these images\n",
    "    if image.mode == 'RGB':\n",
    "        try:\n",
    "            return transform(image)\n",
    "        except:\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def gram_matrix(input):\n",
    "    a, b, c, d = input.size()  # a=batch size(=1)\n",
    "    # b=number of feature maps\n",
    "    # (c,d)=dimensions of a f. map (N=c*d)\n",
    "\n",
    "    features = input.view(a * b, c * d)  # resise F_XL into \\hat F_XL\n",
    "\n",
    "    G = torch.mm(features, features.t())  # compute the gram product\n",
    "\n",
    "    # we 'normalize' the values of the gram matrix\n",
    "    # by dividing by the number of element in each feature maps.\n",
    "    return G.div(a * b * c * d)\n",
    "\n",
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "def get_style_model_and_losses(style_img, content_img, image_name, cnn = models.vgg19(pretrained=True).features.to(device).eval(), \n",
    "                               normalization_mean = torch.tensor([0.485, 0.456, 0.406]).to(device),\n",
    "normalization_std = torch.tensor([0.229, 0.224, 0.225]).to(device),  content_layers=['conv_4'],  \n",
    "                               style_layers=['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']):\n",
    "    # This function is a modified version of the function in Alexis' tutorial. It creates a network,\n",
    "    # and uses the network to create and save content and style matrices through calls to the get_content_matrices\n",
    "    # and get_style_matrices\n",
    "    # normalization module\n",
    "    normalization = Normalization(normalization_mean, normalization_std).to(device)\n",
    "\n",
    "    # just in order to have an iterable access to or list of content/syle\n",
    "    # losses\n",
    "    content_matrices = list()\n",
    "    style_matrices = list()\n",
    "\n",
    "    # assuming that cnn is a nn.Sequential, so we make a new nn.Sequential\n",
    "    # to put in modules that are supposed to be activated sequentially\n",
    "    model = nn.Sequential(normalization)\n",
    "\n",
    "    i = 0  # increment every time we see a conv\n",
    "    for layer in cnn.children():\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            i += 1\n",
    "            name = 'conv_{}'.format(i)\n",
    "        elif isinstance(layer, nn.ReLU):\n",
    "            name = 'relu_{}'.format(i)\n",
    "            # The in-place version doesn't play very nicely with the ContentLoss\n",
    "            # and StyleLoss we insert below. So we replace with out-of-place\n",
    "            # ones here.\n",
    "            layer = nn.ReLU(inplace=False)\n",
    "        elif isinstance(layer, nn.MaxPool2d):\n",
    "            name = 'pool_{}'.format(i)\n",
    "        elif isinstance(layer, nn.BatchNorm2d):\n",
    "            name = 'bn_{}'.format(i)\n",
    "        else:\n",
    "            raise RuntimeError('Unrecognized layer: {}'.format(layer.__class__.__name__))\n",
    "            \n",
    "        model.add_module(name, layer)\n",
    "        \n",
    "        if name in content_layers:\n",
    "            # add content loss:\n",
    "            target = model(content_img).detach()\n",
    "            content_matrices.append(get_content_matrix(target, image_name + '_' + name))\n",
    "            \n",
    "\n",
    "        if name in style_layers:\n",
    "            # add style loss:\n",
    "            target_feature = model(style_img).detach()\n",
    "            style_matrices.append(get_style_matrix(target_feature, image_name + '_' + name))\n",
    "            \n",
    "    return style_matrices, content_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set the folder that contains the album images\n",
    "album_images = 'Images/'\n",
    "count = 0\n",
    " \n",
    "for album_index in oramas_data['album_index'].unique():\n",
    "    # Get one index from the Oramas data corresponding to this album index. This will allow us to load the relevant image\n",
    "    img = oramas_data[oramas_data['album_index'] == album_index].index[0]\n",
    "    \n",
    "    image = get_image('%s/%s' % (album_images, img+'.jpg'))\n",
    "    if image == None:\n",
    "        print('Error getting image, skipping')\n",
    "        continue\n",
    "    image = torch.unsqueeze(image, 0)\n",
    "    # Call the function to create content and style matrices for the image. Pass the name to be used to save the\n",
    "    # matrices. This name is the album index.\n",
    "    get_style_model_and_losses(image, image, album_index)\n",
    "    count += 1\n",
    "    if count % 100 == 0:\n",
    "        print(count)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( content_ratios, open(\"content_ratios_50_element_pca\", \"wb\" ) )\n",
    "pickle.dump( style_ratios, open(\"style_ratios_15_element_pca\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the distribution information for the content ratios. We want to determine that each of our images\n",
    "# has at least 95% of variance explained\n",
    "stats.describe(content_ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same for the style ratios\n",
    "stats.describe(style_ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the histogram of the content ratios\n",
    "plt.hist(content_ratios, bins=50)\n",
    "plt.gca().set(title='Frequency Histogram', ylabel='Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same for the style ratios\n",
    "plt.hist(style_ratios, bins=50)\n",
    "plt.gca().set(title='Frequency Histogram', ylabel='Frequency')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
